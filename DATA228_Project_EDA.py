# -*- coding: utf-8 -*-
"""DATA228_Project_EDA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12oEP0fDxp9ZcYr-r6vToFN4eIDUvqUNv

Hi, please take read these rescources.
1. A guide of NDBC data.
https://www.ndbc.noaa.gov/docs/ndbc_web_data_guide.pdf

2. The data of Station 46026 (San Francisco)
https://www.ndbc.noaa.gov/station_history.php?station=46026

3. The desciprtion of features https://www.ndbc.noaa.gov/faq/measdes.shtml

---

# The first part is analyzing the wind data for 7 stations. Only 7 of the 18 stations have records of wind speed, peak gust speed, and sea-level pressure.

## 1. Read the dataset
"""

import pandas as pd

# List of station IDs
station_ids = [46027, 46244, 46213, 46014, 46013, 46026, 46042,
               46239, 46028, 46011, 46218, 46054, 46053, 46251, 46025,
               46258, 46232]

# Initialize an empty dataframe
df_buoy = pd.DataFrame()

# Base URL
base_url = "https://www.ndbc.noaa.gov/data/stdmet/Sep/{}.txt"

# Loop through each station ID to fetch the data and append to the main dataframe
for station_id in station_ids:
    url = base_url.format(station_id)
    try:
        # Read data from the URL
        temp_df = pd.read_csv(url, delim_whitespace=True, skiprows=[1])

        # You can add a new column to track the station ID if needed
        temp_df['Station_ID'] = station_id

        # Append the data to the main dataframe
        df_buoy = df_buoy.append(temp_df, ignore_index=True)
    except:
        print(f"Failed to fetch data for station ID: {station_id}")

# Display the combined dataframe
df_buoy

# Rename columns based on provided descriptions
df_buoy.rename(columns={
    '#YY': 'Year',
    'MM': 'Month',
    'DD': 'Day',
    'hh': 'Hour',
    'mm': 'Minute',
    'WDIR': 'Wind_Direction',
    'WSPD': 'Wind_Speed',
    'GST': 'Peak_Gust_Speed',
    'WVHT': 'Wave_Height',
    'DPD': 'Dominant_Wave_Period',
    'APD': 'Average_Wave_Period',
    'MWD': 'Dominant_Wave_Direction',
    'PRES': 'Sea_Level_Pressure',
    'ATMP': 'Air_Temperature',
    'WTMP': 'Sea_Surface_Temperature',
    'DEWP': 'Dewpoint_Temperature',
    'VIS': 'Station_Visibility',
    'PTDY': 'Pressure_Tendency',
    'TIDE': 'Water_Level'
}, inplace=True)

df_buoy.head()

"""## 2. Handle the missing value"""

import numpy as np

# Define the missing value patterns
missing_patterns = [99.00, 999, 999.0, 99.0]

# Loop through each column and replace each pattern with NaN
for column in df_buoy.columns:
    for pattern in missing_patterns:
        df_buoy[column] = df_buoy[column].replace(to_replace=pattern, value=np.nan, regex=True)

missing_data = df_buoy.isnull().sum()
# Display missing data count for each column
print(missing_data)

# Setting threshold for excessive missing values (e.g., 60%)
threshold = 0.6 * len(df_buoy)

# Drop columns with missing values greater than the threshold
df_buoy.dropna(axis=1, thresh=threshold, inplace=True)

# Drop rows with any missing values
df_buoy.dropna(axis=0, inplace=True)

df_buoy

"""## 3. EDA"""

# Creating a unified timestamp column
df_buoy['Timestamp'] = pd.to_datetime(df_buoy[['Year', 'Month', 'Day', 'Hour', 'Minute']])
df_buoy.head()

print(df_buoy.dtypes)

# Display summary statistics
print(df_buoy.describe())

unique_stations = df_buoy['Station_ID'].unique()
unique_stations

import matplotlib.pyplot as plt
import seaborn as sns

# Setting style
sns.set_style("whitegrid")

# List of unique station IDs
unique_stations = df_buoy['Station_ID'].unique()

# Let's say we want 2 stations per row
num_stations_per_row = 2

for i in range(0, len(unique_stations), num_stations_per_row):
    fig, axes = plt.subplots(2, num_stations_per_row, figsize=(15, 12))

    for j in range(num_stations_per_row):
        if i + j < len(unique_stations):  # Check so we don't go out of bounds
            station = unique_stations[i + j]
            station_data = df_buoy[df_buoy['Station_ID'] == station]

            # First chart: Wind Speed and Peak Gust Speed
            axes[0, j].plot(station_data['Timestamp'], station_data['Wind_Speed'], label='Wind Speed')
            axes[0, j].plot(station_data['Timestamp'], station_data['Peak_Gust_Speed'], label='Peak Gust Speed')
            axes[0, j].set_title(f'Station {station}')
            axes[0, j].set_ylabel('Speed (m/s)')
            axes[0, j].legend()

            # Second chart: Sea Level Pressure
            axes[1, j].plot(station_data['Timestamp'], station_data['Sea_Level_Pressure'], color='green', label='Sea Level Pressure')
            axes[1, j].set_xlabel('Time')
            axes[1, j].set_ylabel('Pressure (hPa)')
            axes[1, j].legend()

    # Display the plots
    plt.tight_layout()
    plt.show()

# Grouping by day and Station_ID, then calculating the mean for each column
daily_averages = df_buoy.groupby([df_buoy['Timestamp'].dt.date, 'Station_ID']).mean().reset_index()

# List of unique station IDs
unique_stations = df_buoy['Station_ID'].unique()

# For each unique station, we'll plot the daily averages
for station in unique_stations:
    station_data = daily_averages[daily_averages['Station_ID'] == station]

    plt.figure(figsize=(15, 6))
    plt.plot(station_data['Timestamp'], station_data['Wind_Speed'], label='Average Daily Wind Speed', color='blue')
    plt.plot(station_data['Timestamp'], station_data['Peak_Gust_Speed'], label='Average Daily Peak Gust Speed', color='red', linestyle='--')
    plt.title(f'Daily Trends for Wind Speed and Peak Gust Speed for Station {station}')
    plt.xlabel('Date')
    plt.ylabel('Speed (m/s)')
    plt.legend()
    plt.tight_layout()
    plt.show()

# Create a pivot table with hours as columns and days as rows
# For each unique station, we'll create a heatmap
for station in unique_stations:
    station_data = df_buoy[df_buoy['Station_ID'] == station]
    pivot_table = station_data.pivot_table(index=station_data['Timestamp'].dt.date,
                                           columns=station_data['Timestamp'].dt.hour,
                                           values='Wind_Speed',
                                           aggfunc='mean')

    plt.figure(figsize=(15, 10))
    sns.heatmap(pivot_table, cmap='coolwarm', cbar_kws={'label': 'Wind Speed (m/s)'})
    plt.title(f'Heatmap of Wind Speed by Hour and Day for Station {station}')
    plt.xlabel('Hour of the Day')
    plt.ylabel('Date')
    plt.tight_layout()
    plt.show()

"""## 4. Anomaly Detection"""

window_size = 7  # 7 days window

# For each unique station, we'll create an anomaly detection plot
for station in unique_stations:
    station_data = df_buoy[df_buoy['Station_ID'] == station]
    daily_averages = station_data.resample('D', on='Timestamp').mean()

    rolling_mean = daily_averages['Wind_Speed'].rolling(window=window_size).mean()
    rolling_std = daily_averages['Wind_Speed'].rolling(window=window_size).std()

    # Detect anomalies
    anomalies = daily_averages[(daily_averages['Wind_Speed'] > rolling_mean + 2*rolling_std) |
                               (daily_averages['Wind_Speed'] < rolling_mean - 2*rolling_std)]

    # Plot
    plt.figure(figsize=(15, 6))
    plt.plot(daily_averages.index, daily_averages['Wind_Speed'], label='Daily Average Wind Speed')
    plt.plot(rolling_mean, color='red', label='Rolling Mean')
    plt.scatter(anomalies.index, anomalies['Wind_Speed'], color='k', s=50, label='Anomaly')
    plt.fill_between(daily_averages.index, rolling_mean+2*rolling_std, rolling_mean-2*rolling_std, color='yellow', alpha=0.4)
    plt.title(f'Anomaly Detection for Daily Average Wind Speed at Station {station}')
    plt.legend()
    plt.tight_layout()
    plt.show()

"""## 5. Trend Analysis"""

from statsmodels.tsa.seasonal import seasonal_decompose

for station in unique_stations:
    station_data = df_buoy[df_buoy['Station_ID'] == station]
    daily_averages = station_data.resample('D', on='Timestamp').mean()

    # Decomposing the 'Wind_Speed' column, while interpolating missing values
    decomposition = seasonal_decompose(daily_averages['Wind_Speed'].interpolate(method='linear'), model='additive')

    # Plotting the decomposed components
    plt.figure(figsize=(15, 10))
    plt.suptitle(f'Seasonal Decomposition for Station {station}', fontsize=16)
    decomposition.plot()
    plt.show()

"""# The Second part is analyzing the wave data for 18 stations.

## 1. Read the dataset
"""

import pandas as pd

# List of station IDs
station_ids = [46027, 46244, 46213, 46014, 46013, 46026, 46042,
               46239, 46028, 46011, 46218, 46054, 46053, 46251, 46025,
               46258, 46232]

# Initialize an empty dataframe
df = pd.DataFrame()

# Base URL
base_url = "https://www.ndbc.noaa.gov/data/stdmet/Sep/{}.txt"

# Loop through each station ID to fetch the data and append to the main dataframe
for station_id in station_ids:
    url = base_url.format(station_id)
    try:
        # Read data from the URL
        temp_df = pd.read_csv(url, delim_whitespace=True, skiprows=[1])

        # You can add a new column to track the station ID if needed
        temp_df['Station_ID'] = station_id

        # Append the data to the main dataframe
        df = df.append(temp_df, ignore_index=True)
    except:
        print(f"Failed to fetch data for station ID: {station_id}")

# Display the combined dataframe
df

# Drop the specified columns
cols_to_drop = ['WDIR', 'WSPD', 'GST', 'PRES', 'ATMP', 'WTMP', 'DEWP', 'VIS', 'TIDE']
df = df.drop(columns=cols_to_drop)

print(df)

missing_patterns = [99.00, 999, 999.0, 99.0]

for column in df.columns:
    for pattern in missing_patterns:
        df[column] = df[column].replace(to_replace=pattern, value=np.nan, regex=True)

missing_data = df.isnull().sum()
print(missing_data)

# Drop rows with any missing values
df.dropna(axis=0, inplace=True)

df

unique_stations = df['Station_ID'].unique()
unique_stations

# Rename columns based on provided descriptions
df.rename(columns={
    '#YY': 'Year',
    'MM': 'Month',
    'DD': 'Day',
    'hh': 'Hour',
    'mm': 'Minute',
    'WVHT': 'Wave_Height',
    'DPD': 'Dominant_Wave_Period',
    'APD': 'Average_Wave_Period',
    'MWD': 'Dominant_Wave_Direction',
}, inplace=True)

df

# Creating a unified timestamp column
df['Timestamp'] = pd.to_datetime(df[['Year', 'Month', 'Day', 'Hour', 'Minute']])
df.head()

# Assuming the cleaned data is named df_cleaned
df['Timestamp'] = pd.to_datetime(df['Timestamp'])
unique_stations = df['Station_ID'].unique()

# Settings
sns.set_style("whitegrid")
stations_per_row = 3
num_rows = int(np.ceil(len(unique_stations) / stations_per_row))

fig, axs = plt.subplots(num_rows, stations_per_row, figsize=(20, 6 * num_rows))

for idx, station in enumerate(unique_stations):
    row = idx // stations_per_row
    col = idx % stations_per_row

    # Handling the subplot axis for single and multiple rows
    if num_rows > 1:
        ax = axs[row, col]
    else:
        ax = axs[col]

    station_data = df[df['Station_ID'] == station]
    daily_averages = station_data.resample('D', on='Timestamp').mean()

    daily_averages['Wave_Height'].plot(ax=ax, label='Average Daily Wave Height', color='blue')
    daily_averages['Dominant_Wave_Period'].plot(ax=ax, label='Average Daily Dominant Wave Period', color='red', linestyle='--')
    daily_averages['Average_Wave_Period'].plot(ax=ax, label='Average Daily Average Wave Period', color='green', linestyle='-.')

    ax.set_title(f'Station {station}')
    ax.set_xlabel('Date')
    ax.legend()

# Removing any unused subplots
if len(unique_stations) % stations_per_row != 0:
    for j in range(len(unique_stations) % stations_per_row, stations_per_row):
        fig.delaxes(axs[num_rows-1, j])

plt.tight_layout()
plt.show()

sns.set_style("whitegrid")
stations_per_row = 3
num_rows = int(np.ceil(len(unique_stations) / stations_per_row))

fig, axs = plt.subplots(num_rows, stations_per_row, figsize=(20, 6 * num_rows))

for idx, station in enumerate(unique_stations):
    row = idx // stations_per_row
    col = idx % stations_per_row

    # Handling the subplot axis for single and multiple rows
    if num_rows > 1:
        ax = axs[row, col]
    else:
        ax = axs[col]

    station_data = df[df['Station_ID'] == station]

    pivot_table = station_data.pivot_table(index=station_data['Timestamp'].dt.date,
                                           columns=station_data['Timestamp'].dt.hour,
                                           values='Wave_Height',
                                           aggfunc='mean')

    sns.heatmap(pivot_table, ax=ax, cmap='coolwarm', cbar_kws={'label': 'Wave Height'})
    ax.set_title(f'Station {station}')
    ax.set_xlabel('Hour of the Day')
    ax.set_ylabel('Date')

# Removing any unused subplots
if len(unique_stations) % stations_per_row != 0:
    for j in range(len(unique_stations) % stations_per_row, stations_per_row):
        fig.delaxes(axs[num_rows-1, j])

plt.tight_layout()
plt.show()

# Settings
window_size = 7  # 7 days window for rolling stats
stations_per_row = 3
num_rows = int(np.ceil(len(unique_stations) / stations_per_row))

fig, axs = plt.subplots(num_rows, stations_per_row, figsize=(20, 6 * num_rows))

for idx, station in enumerate(unique_stations):
    row = idx // stations_per_row
    col = idx % stations_per_row

    # Handling the subplot axis for single and multiple rows
    if num_rows > 1:
        ax = axs[row, col]
    else:
        ax = axs[col]

    station_data = df[df['Station_ID'] == station]
    daily_averages = station_data.resample('D', on='Timestamp').mean()

    # Calculate rolling mean and std
    rolling_mean = daily_averages['Wave_Height'].rolling(window=window_size).mean()
    rolling_std = daily_averages['Wave_Height'].rolling(window=window_size).std()

    # Detect anomalies
    anomalies = daily_averages[(daily_averages['Wave_Height'] > rolling_mean + 2*rolling_std) |
                               (daily_averages['Wave_Height'] < rolling_mean - 2*rolling_std)]

    # Plotting
    ax.plot(daily_averages.index, daily_averages['Wave_Height'], label='Daily Average Wave Height')
    ax.plot(rolling_mean, color='red', label='Rolling Mean')
    ax.scatter(anomalies.index, anomalies['Wave_Height'], color='k', s=50, label='Anomaly')
    ax.fill_between(daily_averages.index, rolling_mean+2*rolling_std, rolling_mean-2*rolling_std, color='yellow', alpha=0.4)
    ax.set_title(f'Station {station}: Anomaly Detection')
    ax.legend()

# Removing any unused subplots
if len(unique_stations) % stations_per_row != 0:
    for j in range(len(unique_stations) % stations_per_row, stations_per_row):
        fig.delaxes(axs[num_rows-1, j])

plt.tight_layout()
plt.show()

